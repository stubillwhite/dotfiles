# vim:fdm=marker

# Printing                                                                  {{{1
# ==============================================================================

function start-printer-service() {
    if [[ -n $(ps aux | grep -i 'UniFLOW SmartClient' | grep -v 'grep') ]]; then 
        echo 'Printer service already running'
    else 
        echo 'Starting printer service'
        alias start-printer-service="/Applications/uniFLOW\ SmartClient.app/Contents/MacOS/SmartClient &"
    fi
}

# Certificates                                                              {{{1
# ==============================================================================

export SSL_CERT_FILE=~/dev/certificates/ZscalerRootCertificate-2048-SHA256.crt

#export SSL_CERT_FILE="${SSL_CERT_FILE}"        # openssl
#export REQUESTS_CA_BUNDLE="${SSL_CERT_FILE}"   # requests
#export AWS_CA_BUNDLE="${SSL_CERT_FILE}"        # botocore
#export CURL_CA_BUNDLE="${SSL_CERT_FILE}"       # curl
#export HTTPLIB2_CA_CERTS="${SSL_CERT_FILE}"    # httplib2
#export NODE_EXTRA_CA_CERTS="${SSL_CERT_FILE}"  # node

# SSH                                                                       {{{1
# ==============================================================================

function ssh-find-username() {
    if [[ $# -ne 1 ]] ; then
        echo 'Usage: ssh-find-username ADDRESS'
        return 1
    fi

    local addr=$1

    local USERNAMES=(
        centos
        ec2-user
        hadoop
        admin
    )

    for username in $USERNAMES[@]; do
        printf "${COLOR_CLEAR_LINE}Trying %s" "${username}"
        capturedOutput=$(ssh -q -o ConnectTimeout=5 "${username}@${addr}" "echo 'Test command'" 2>&1)
        exitCode=$?
        if [ $exitCode -eq 0 ]; then
            printf "${COLOR_CLEAR_LINE}${username}\n"
            return 0
        fi
    done
    printf "${COLOR_CLEAR_LINE}Unable to find username for ${addr}\n"
    return 1
}

# Kubernetes                                                                {{{1
# ==============================================================================

source <(kubectl completion zsh)

source <(stern --completion=zsh)

function aws-recs-login() {
    if [[ $# -ne 1 ]]; then
        echo "Usage: aws-recs-login (dev|staging|live)"
    else
        local recsEnv=$1

        case "${recsEnv}" in
            dev*)
                aws-sso-login recs-dev
            ;;

            staging*)
                aws-sso-login recs-dev
            ;;

            live*)
                aws-sso-login recs-prod
            ;;

            *)
                echo "ERROR: Unrecognised environment ${recsEnv}"
                return -1
            ;;
        esac
    fi
}
compdef "_arguments \
    '1:environment arg:(dev staging live)'" \
    aws-recs-login

function jira-branch() {
    git checkout -b $1
}
compdef _jira-branch jira-branch

function recs-get-k8s() {
    if [[ $# -ne 2 ]] ; then
        echo "Usage: recs-get-k8s (dev|live) (util|main)"
    else
        local recsEnv=$1
        local recsSubEnv=$2
        aws s3 cp s3://com-elsevier-recs-${recsEnv}-certs/eks/recs-eks-${recsSubEnv}-${recsEnv}.conf ~/.kube/
        export KUBECONFIG=~/.kube/recs-eks-${recsSubEnv}-${recsEnv}.conf

        # TODO: We need to upgrade our K8s config
        # https://github.com/fluxcd/flux2/issues/2817
        # echo
        # echo 'WARNING: My tools are running with newer clients than our servers; patching protocol'
        # local oldProtocol="client.authentication.k8s.io\/v1alpha1"
        # local newProtocol="client.authentication.k8s.io\/v1beta1"
        # gsed -i "s/${oldProtocol}/${newProtocol}/g" $HOME/.kube/recs-eks-${recsSubEnv}-${recsEnv}.conf
    fi
}
compdef "_arguments \
    '1:environment arg:(dev live)' \
    '2:sub-environment arg:(util main)'" \
    recs-get-k8s

function recs-k9s-dev() {
    aws-sso-login recs-dev
    recs-get-k8s dev main
    k9s
}

function recs-k9s-dev-util() {
    aws-sso-login recs-dev
    recs-get-k8s dev util
    k9s
}

function recs-k9s-staging() {
    aws-sso-login recs-dev
    recs-get-k8s staging main
    k9s
}

function recs-k9s-live() {
    aws-sso-login recs-prod
    recs-get-k8s live main
    k9s
}

function recs-k9s-live-util() {
    aws-sso-login recs-prod
    recs-get-k8s live util
    k9s
}

function recs-api-versions() {
    if [[ $# -ne 1 ]] ; then
        echo "Usage: recs-api-versions (dev|live)"
    else
        local recsEnv=$1

        [[ ${recsEnv} == "dev" ]]  && aws-sso-login recs-dev  > /dev/null
        [[ ${recsEnv} == "live" ]] && aws-sso-login recs-prod > /dev/null

        recs-get-k8s $1 main > /dev/null

        kubectl get po -A -o json \
            | jq -r ".items[].status.containerStatuses[].image" \
            | grep "api" \
            | gsed -e 's/^.*\/api\///g' \
            | sort \
            | uniq \
            | tr ':' ',' \
            | tabulate-by-comma
    fi
}
compdef "_arguments \
    '1:environment arg:(dev live)'" \
    recs-api-versions

function recs-get-emr-logs() {
    setopt local_options BASH_REMATCH
    if [[ $# -ne 1 ]]; then
        echo "Usage: recs-get-emr-logs S3_PATH"
    else
        local s3Path=$1
        if [[ ${s3Path} =~ "com-elsevier-recs-(.+)-logs" ]]; then
            local recsEnv=$BASH_REMATCH[2]
            case "${recsEnv}" in
                dev*)
                    aws-sso-login recs-dev
                    ;;

                staging*)
                    aws-sso-login recs-dev
                    ;;

                live*)
                    aws-sso-login recs-prod
                    ;;

                *)
                    echo "ERROR: Unrecognised environment ${recsEnv}"
                    return -1
                    ;;
            esac

            # Clean old logs if they exist
            rm -rf ~/trash/logs
            mkdir ~/trash/logs
            pushd ~/trash/logs

            # Copy the logs
            aws s3 cp --recursive "${s3Path}" .

            # Unzip
            while read -r line; do
                echo "$line"
                gunzip "$line"
            done < <(find . -name "*.gz")

            # Start nvim, search for error, view only results
            nvim -c 'Ack! error' -c 'only'
        else
            echo "ERROR: Unrecognised path ${s3Path}"
        fi
    fi
}

# APIs
# ==============================================================================

function recs-api-statuses() {
    local APIS=(
        article-recommendations-tailored.api
        fi-recommender.api
        library-stats.api
        raven-email-sent-stats.api
        recs-events-service.api
        recs-focus-stats.api
        recs-reviewers-recommender.api
        sd-article-recommendations.api
        sd-hpcc-related-articles.api
        sd-logged-email-events.api
        sd-related-articles.api
        sd-user-activity.api
        sd-user-recommendations.api
    )
    (
        #printf "%s|%s|%s|%s|%s\n" "API" "Dev" "Staging" "Live" "Dev URL"
        printf "%s|%s|%s|%s\n" "API" "Dev" "Live" "Dev URL"
        for API in "${APIS[@]}"
        do
            printf "%s" "${API}"
            curl -s -w "%{http_code}" -o >(printf "|%s") -X GET "https://${API}.dev.recs.d.elsevier.com/api" 2>/dev/null
            #curl -s -w "%{http_code}" -o >(printf "|%s") -X GET "https://${API}.staging.recs.d.elsevier.com/api" 2>/dev/null
            curl -s -w "%{http_code}" -o >(printf "|%s") -X GET "https://${API}.recs.d.elsevier.com/api" 2>/dev/null
            printf "|%s" "https://${API}.dev.recs.d.elsevier.com/api"
            printf "\n"
        done
    ) | column -t -s '|' \
      | highlight red   '\b[045][0-9]\+\b' \
      | highlight green '\b[2][0-9]\+\b'
}

function recs-certificate-statuses() {
    local DOMAINS=(
        article-recommendations-tailored.api
        fi-recommender.api
        library-stats.api
        raven-email-sent-stats.api
        recs-events-service.api
        recs-focus-stats.api
        recs-reviewers-recommender.api
        sd-article-recommendations.api
        sd-hpcc-related-articles.api
        sd-logged-email-events.api
        sd-related-articles.api
        sd-user-activity.api
        sd-user-recommendations.api
        recs.d.elsevier.com
    )
    (
        printf "%s|%s\n" "Domain" "Expiry"
        for DOMAIN in "${DOMAINS[@]}"
        do
            local expiry=$(certificate-expiry-curl "${DOMAIN}.recs.d.elsevier.com" | gsed 's/^.* date: \(.*\)/\1/g')
            printf "%s|%s\n" "${DOMAIN}" "${expiry}"
        done
    ) | column -t -s '|'
}


function dkp-api-statuses() {
    local APIS=(
        https://data.elsevier.com/api/
        https://data-dev.elsevier.com/api/
        https://data-sandbox.elsevier.com/api/
        https://collections.elsevier.com/
        https://collections-dev.elsevier.com/
        https://imagefinder.elsevier.com/
        https://imagefinder-dev.elsevier.com/
        https://prod.topbraid.elsevier.net
    )
    (
        printf "%s|%s|%s|%s\n" "API" "Status" "Endpoint"
        for API in "${APIS[@]}"
        do
            local url="${API}"
            local host=$(echo ${API} | get-hostname)
            printf "%s" "${host}"
            curl --connect-timeout 5 -s -w "%{http_code}" -o >(printf "|%s") -X GET "${url}" 2>/dev/null
            printf "|%s" "${url}"
            printf "\n"
        done
    ) | column -t -s '|' \
      | highlight red   '\b[045][0-9]\+\b' \
      | highlight green '\b[2][0-9]\+\b'
}

function dkp-certificate-statuses() {
    local DOMAINS=(
        data-dev.elsevier.com
        data-sandbox.elsevier.com
        data.elsevier.com
        collections.elsevier.com
        imagefinder.elsevier.com
        prod.topbraid.elsevier.net
    )
    (
        printf "%s|%s\n" "Domain" "Expiry"
        for DOMAIN in "${DOMAINS[@]}"
        do
            local expiry=$(certificate-expiry-curl ${DOMAIN} | gsed 's/^.* date: \(.*\)/\1/g')
            printf "%s|%s\n" "${DOMAIN}" "${expiry}"
        done
    ) | column -t -s '|'
}

function recs-fi-performance() {
    hey -n 20 -c 3 -H 'Accept: application/json' -m GET 'https://fi-recommender.api.recs.d.elsevier.com/api/fi-recommendations/webuser/37325013/author/23982012500'
}

# Data pipelines
# ==============================================================================

function recs-reviewers-evise-dumps() {
    if [[ $# -ne 1 ]] ; then
        echo "Usage: recs-reviewers-evise-dumps (dev|live)"
    else
        local recsEnv=$1

        [[ ${recsEnv} == "dev" ]]  && aws-sso-login recs-dev  > /dev/null
        [[ ${recsEnv} == "live" ]] && aws-sso-login recs-prod > /dev/null

        manuscriptsPath="s3://com-elsevier-recs-${recsEnv}-reviewers/raw/batched-manuscripts"
        reviewersPath="s3://com-elsevier-recs-${recsEnv}-reviewers/raw/batched-reviewers"

        manuscriptsTimestamp=$(aws s3 ls "${manuscriptsPath}/" | tail -n 1 | awk '{ print $2; }' | gsed 's/\///g')
        reviewersTimestamp=$(aws s3 ls "${reviewersPath}/" | tail -n 1 | awk '{ print $2; }' | gsed 's/\///g')

        (
            printf "Data set|Timestamp|Count\n"
            printf "%s|%s|%s\n" "Reviewers"   ${reviewersTimestamp}   $(aws s3 ls "${reviewersPath}/${reviewersTimestamp}/data/"     | wc -l)
            printf "%s|%s|%s\n" "Manuscripts" ${manuscriptsTimestamp} $(aws s3 ls "${manuscriptsPath}/${manuscriptsTimestamp}/data/" | wc -l)
        ) | column -t -s '|'
    fi
}
compdef "_arguments \
    '1:environment arg:(dev live)'" \
    recs-reviewers-evise-dumps

# Dashboards
# ==============================================================================

function recs-reviewers-dashboards() {
    # Obtain the dashboard's GUID:
    #  - Click the "tag" icon by the dashboard's name to access the See metadata and manage tags modal and see the dashboard's GUID.
    local dashboards=(
        "MjA5OTI0M3xWSVp8REFTSEJPQVJEfGRhOjgyNzA4ODk"   # https://onenr.io/0OQM7dGXeRG
        "MjA5OTI0M3xWSVp8REFTSEJPQVJEfGRhOjgyNzA4ODI"   # https://onenr.io/0PwJxmd6Dj7
        "MjA5OTI0M3xWSVp8REFTSEJPQVJEfGRhOjgyNzA4ODQ"   # https://onenr.io/0EjOE9G6Dw6
        "MjA5OTI0M3xWSVp8REFTSEJPQVJEfGRhOjgyNzA4ODY"   # https://onenr.io/08wpgE036QO
    )

    for dashboard in $dashboards[@]; do
        # Find the sub-dashboards in the main page
        subDashboardsResponse=$(curl -s https://api.newrelic.com/graphql \
            -H 'Content-Type: application/json' \
            -H "API-Key: ${SECRET_NEWRELIC_API_KEY}" \
            --data-binary '{"query":"{ actor { entity(guid: \"'${dashboard}'\") { ... on DashboardEntity { guid name pages { guid name } } } } } ", "variables":""}'
        )

        subDashboard=$(echo "${subDashboardsResponse}" | jq -r '.data.actor.entity.pages[0].guid')

        # Generate a new URL for the current data
        dashboardUrlResponse=$(curl -s https://api.newrelic.com/graphql \
            -H 'Content-Type: application/json' \
            -H "API-Key: ${SECRET_NEWRELIC_API_KEY}" \
            --data-binary '{"query":"mutation { dashboardCreateSnapshotUrl(guid: \"'${subDashboard}'\") } ", "variables":""}'
        )

        dashboardUrl=$( \
            echo "${dashboardUrlResponse}" \
            | jq -r '.data.dashboardCreateSnapshotUrl' \
            | sed 's/format=PDF/format=PNG/' \
        )

        # Download and show
        wget -q "${dashboardUrl}" -O - | imgcat -R
    done
}

function recs-reviewers-glue-job-status {
    aws glue get-job-runs --job-name coalesce-daily-recommendations \
        | jq -r '["Completed", "State"], (.JobRuns | sort_by(.CompletedOn) | .[] | [.CompletedOn, .JobRunState]) | @csv' \
        | strip-quotes \
        | tabulate-by-comma \
        | highlight green SUCCEEDED \
        | highlight red FAILED
}

# SonarQube                                                                 {{{1
# ==============================================================================

function sonarqube-run() {
    if [[ $# -ne 1 ]] ; then
        echo "Usage: sonarqube-run TOKEN"
        echo "  Token can be retrieved from recs-secrets"
    else
        local sonarToken=$1

        local sonarServer="https://sq.prod.tio.elsevier.systems"
        local sonarScannerHome=$(full-path "$(which sonar-scanner | xargs greadlink -f | xargs dirname)/..")
        local sonarOpts="-Dsonar.host.url=\"${sonarServer}\" -DsonarScanner.home=\"${sonarScannerHome}\" -Dsonar.login=\"${sonarToken}\""

        echo "NOTE: Ensure you have first generated coverage with 'sbt clean coverage test coverageReport'"
        echo "Using sonar-scanner at ${sonarScannerHome}"
        SONAR_SCANNER_HOME="${sonarScannerHome}" sbt -Dsonar.host.url="${sonarServer}" -Dsonar.login="${sonarToken}" sonarScan
    fi
}

# Conda                                                                     {{{1
# ==============================================================================

function conda-insinuate() {
    # >>> conda initialize >>>
    # !! Contents within this block are managed by 'conda init' !!
    __conda_setup="$('/usr/local/Caskroom/miniconda/base/bin/conda' 'shell.zsh' 'hook' 2> /dev/null)"
    if [ $? -eq 0 ]; then
        eval "$__conda_setup"
    else
        if [ -f "/usr/local/Caskroom/miniconda/base/etc/profile.d/conda.sh" ]; then
            . "/usr/local/Caskroom/miniconda/base/etc/profile.d/conda.sh"
        else
            export PATH="/usr/local/Caskroom/miniconda/base/bin:$PATH"
        fi
    fi
    unset __conda_setup
    # <<< conda initialize <<<
}

# Newsflo AWS                                                               {{{1
# ==============================================================================

function sshx-tagged-aws-machines() {
    if [[ $# -ne 3 ]] ; then
        echo 'Usage: sshx-tagged-aws-machines TAG'
        return 1
    fi

    declare tag=$1

    echo 'Finding machines'
    machines=($(aws ec2 describe-instances | jq --raw-output '.Reservations[].Instances[]? | select(.State.Name=="running") | select(.Tags[] | select((.Key=="Name") and (.Value=="'$tag'"))) | .NetworkInterfaces[].PrivateIpAddresses[].PrivateIpAddress'))

    echo "Opening SSH to $machines[*]"
    i2cssh $machines[*]
}

function aws-instance-info() {
    local tag=$1

    aws ec2 describe-instances | jq --raw-output '.Reservations[].Instances[]? | select(.Tags[].Value=="'$tag'") | select(.State.Name=="running")'
}
compdef _aws-tag aws-instance-info

# List the values of tagged AWS instances
function aws-tag-values() {
    if [[ $# -ne 1 ]] ; then
        echo 'Usage: aws-tag-values PROFILE REGION KEY'
        return 1
    fi

    local key=$1

    aws ec2 describe-instances | jq --raw-output '.Reservations[].Instances[].Tags[]? | select(.Key=="'$key'") | .Value' | sort | uniq
}
compdef _aws-tag aws-tag-values

# List the IPs for tagged AWS instances
function aws-instance-ips() {
    if [[ $# -ne 1 ]] ; then
        echo 'Usage: aws-instance-ips TAG'
        return 1
    fi

    local tag=$1

    aws ec2 describe-instances | jq --raw-output '.Reservations[].Instances[] | select(.Tags[]?.Value=="'$tag'") | select(.State.Name=="running") | .PrivateIpAddress' | sort | uniq
}
compdef _aws-tag aws-instance-ips

# List the IPs for all AWS instances
function aws-all-instance-ips() {
    local jqScript
    read-heredoc jqScript <<HEREDOC
    [.Reservations[].Instances[]?
        | select(.State.Name=="running")
        | {
             name:         (.Tags | map({key: .Key, value: .Value}) | from_entries | .Name // "-"),
             ipAddress:    .PrivateIpAddress,
             instanceId:   .InstanceId,
             instanceType: .InstanceType,
             imageId:      .ImageId,
             launchTime:   .LaunchTime,
             monitoring:   .Monitoring.State
          }
    ]
    | sort_by(.name)
HEREDOC

    aws ec2 describe-instances \
        | jq -r "${jqScript}" \
        | json-to-csv \
        | strip-quotes \
        | tabulate-by-comma
}

# For each region in AWS, execute the specified function
function aws-for-all-regions() {
    local originalRegion=${AWS_REGION}

    local regions=$(aws ec2 describe-regions | jq -r '.Regions[].RegionName')

    while read -r region; do
        echo "$region"
        export AWS_REGION=${region}
        "$@"
        echo
    done < <(echo ${regions})

    export AWS_REGION=${originalRegion}
}

# List the IPs for all AWS instances in all regions
function aws-all-regions-all-instance-ips() {

    list-instances() {
        aws-all-instance-ips 2>/dev/null
    }

    aws-for-all-regions list-instances
}

# List the DynamoDB tables in all regions
function aws-all-regions-dynamodb-tables() {

    list-tables() {
        aws dynamodb list-tables | jq -r '.'
    }

    aws-for-all-regions list-tables
}

# List the DynamoDB tables in all regions
function aws-all-regions-rds-instances() {

    list-tables() {
        aws rds describe-db-instances | jq -r '.TableNames[]'
    }

    aws-for-all-regions list-tables
}

function aws-all-regions-unattached-ebs-volumes() {

    unnattached-ebs() {
        aws ec2 describe-volumes \
            | jq -r '.Volumes[] | select(.State != "in-use") | [.VolumeId, .State] | @csv' \
            | gsed 's/"//g' \
            | prepend "VolumeId,State" \
            | tabulate-by-comma
    }

    aws-for-all-regions unnattached-ebs
}

# List AWS instance limits
function aws-ec2-instance-limits() {
    aws service-quotas list-service-quotas --service-code ec2 | jq --raw-output '(.Quotas[] | ([.QuotaName, .Value])) | @csv' | column -t -s "," | sed 's/\"//g'
}

function active-directory-service-user-info() {
    if [[ $# -ne 1 ]]; then
        echo "Usage: active-directory-service-user-info USERNAME"
        return 1
    fi
    dscl "/Active Directory/SCIENCE/All Domains" read "/Users/${1}"
    echo "For more detailed information open the 'Directory Utility' app"
}

function active-directory-all-users() {
    dscl . readall /users
}

function active-directory-all-groups() {
    dscl . readall /groups
}

function recs-build-and-publish-jar() {
    sbt-no-test clean assembly

    local assembly=$(find target -type f -name *-assembly-*.jar)
    local appsPath=s3://com-elsevier-recs-live-experiments/stuw-hacked-apps

    if [[ $assembly =~ ".*/(.*)-assembly-.*" ]]
    then
        local prefix="${BASH_REMATCH[2]}"
        local timestamp=$(date +"%Y%m%d-%H%M")
        local name="${prefix}-assembly-hacked-app-${timestamp}.jar"

        aws-sso-login recs-prod
        aws s3 cp "${assembly}" "${appsPath}/${name}"
    else
        echo "Assembly not found"
    fi
}

# Reviewer Recommender                                                      {{{1
# ==============================================================================

function rr-quality-metrics() {
    aws-recs-login live > /dev/null

    latestRun=$(aws s3 ls s3://com-elsevier-recs-live-reviewers/quality-metrics/metrics/demographic-parities/gender/selection-rates/ \
        | tail -n 1 \
        | gsed -r 's/.* PRE (.+)\/$/\1/')

    runId=${latestRun}

    echo ${runId}
    echo

    metrics=(
        demographic-parities
        equal-opportunity-statistics
    )

    characteristics=(
        gender
        geographicallocation
        seniority
    )

    subMetrics=(
        selection-rate-parities
        selection-rates
    )

    for characteristic in "${characteristics[@]}"
    do
        for metric in "${metrics[@]}"
        do
            for subMetric in "${subMetrics[@]}"
            do
                echo "${metric}/${characteristic}/${subMetric}"
                echo
                jsonFile=$(aws s3 ls s3://com-elsevier-recs-live-reviewers/quality-metrics/metrics/${metric}/${characteristic}/${subMetric}/${runId}/data/ \
                    | grep 'part-' \
                    | gsed -r 's/.* (part-.+\.json)$/\1/g')

                if [[ "${subMetric}" = 'selection-rates' ]]; then
                    aws s3 cp s3://com-elsevier-recs-live-reviewers/quality-metrics/metrics/${metric}/${characteristic}/${subMetric}/${runId}/data/${jsonFile} - \
                        | jq -s '.' \
                        | json-to-csv \
                        | strip-quotes \
                        | tabulate-by-comma
                else
                    aws s3 cp s3://com-elsevier-recs-live-reviewers/quality-metrics/metrics/${metric}/${characteristic}/${subMetric}/${runId}/data/${jsonFile} - \
                        | jq '[.label, .selectionRateA.model, .selectionRateParity]' \
                        | jq -s '.' \
                        | json-to-csv \
                        | tail -n +2 \
                        | strip-quotes \
                        | tabulate-by-comma
                fi
                echo
            done
        done
    done
}

function rr-lambda-performance() {
    if [[ $# -ne 1 ]]; then
        echo "Usage: rr-lambda-performance (dev|staging|live)"
        return 1
    fi

    local recsEnv="${1}"
    aws-recs-login "${recsEnv}" > /dev/null

    # Pull out manuscript ID, stage, and duration from the logs
    # Ignore JDBCService initialisation because there is no manuscript ID associated to group by
    local timingData=$(
        awslogs get --no-group --no-stream --timestamp -s "5m" /aws/lambda/recs-reviewers-recommender-lambda-${recsEnv} \
            | grep -e 'Instrumentation\$:' \
            | grep -v 'JDBCService#initialise' \
            | gsed -r 's/^.* \[(.+)\] Instrumentation\$:.+ (.+) - ([0-9]+) ms$/\1 \2 \3/' \
            | (echo 'ManuscriptId' 'Stage' 'Duration' && cat)
    )

    # Sum stages for each manuscript, so multiple invocations of the same operation are added together
    local totalPerStage=$(
        echo ${timingData} \
            | datamash --sort --field-separator=' ' --header-in -g ManuscriptId,Stage sum Duration \
            | (echo 'ManuscriptId' 'Stage' 'TotalDuration' && cat)
    )

    # Display min, mean, and max for each stage
    local statsPerStage=$(
        echo ${totalPerStage} \
            | datamash --sort --field-separator=' ' --header-in --round=1 -g Stage min TotalDuration mean TotalDuration max TotalDuration \
            | (echo 'Stage' 'Min' 'Mean' 'Max' && cat)
    )
    echo ${statsPerStage} | tabulate-by-space

    # Display min, mean, and max total time
    echo
    echo ${statsPerStage} \
        | datamash --sort --field-separator=' ' --header-in --round=1 sum Min sum Mean sum Max \
        | (echo 'TotalMin' 'TotalMean' 'TotalMax' && cat) \
        | tabulate-by-space

    # Display ElasticSearch configuration for reference
    echo
    aws es describe-elasticsearch-domain --domain-name "recs-reviewers" \
        | jq -r '["Instance", "InstanceCount", "Master", "MasterCount", "VolumeType", "IOPs"], (.DomainStatus | [(.ElasticsearchClusterConfig | .InstanceType, .InstanceCount, .DedicatedMasterType, .DedicatedMasterCount), (.EBSOptions | .VolumeType, .Iops)]) | @tsv' \
        | tabulate-by-tab
}
compdef "_arguments \
    '1:environment arg:(dev staging live)'" \
    rr-lambda-performance

function rr-error-queue-depth-live() {
    aws-recs-login live > /dev/null

    aws sqs get-queue-attributes \
        --queue-url https://sqs.us-east-1.amazonaws.com/589287149623/recs_rev_recommender_lambda_errors_dlq \
        --attribute-names All \
        | jq -r '.Attributes.ApproximateNumberOfMessages'
}

function rr-lambda-iterator-age() {
    if [[ $# -ne 1 ]]; then
        echo "Usage: rr-lambda-iterator-age (dev|staging|live)"
        return 1
    fi

    local recsEnv="${1}"
    aws-recs-login "${recsEnv}" > /dev/null

    echo 'Iterator age at' $(date --iso-8601=seconds)
    echo
    aws cloudwatch get-metric-statistics \
        --namespace 'AWS/Lambda' \
        --dimensions Name=FunctionName,Value=recs-reviewers-recommender-lambda-${recsEnv} \
        --metric-name 'IteratorAge' \
        --start-time $(date --iso-8601=seconds --date='45 minutes ago') \
        --end-time   $(date --iso-8601=seconds) \
        --period 300 \
        --statistics Maximum \
        | jq -r '["Time", "Seconds", "Minutes", "Hours"], (.Datapoints | sort_by(.Timestamp) | .[] | [.Timestamp, .Maximum/1000, .Maximum/(60 * 1000), .Maximum/(60 * 60 * 1000)]) | @tsv' \
        | tabulate-by-tab
}
compdef "_arguments \
    '1:environment arg:(dev staging live)'" \
    rr-lambda-iterator-age

function rr-data-pump-lambda-submitted-manuscripts() {
    if [[ $# -ne 1 ]]; then
        echo "Usage: rr-data-pump-lambda-submitted-manuscripts (dev|staging|live)"
        return 1
    fi

    local recsEnv="${1}"
    aws-recs-login "${recsEnv}" > /dev/null

    local KINESIS_STREAM_NAME="recs-reviewers-submitted-manuscripts-stream-${recsEnv}"

    local SHARD_ITERATOR=$(aws kinesis get-shard-iterator \
        --shard-id shardId-000000000000 \
        --shard-iterator-type TRIM_HORIZON \
        --stream-name $KINESIS_STREAM_NAME \
        --query 'ShardIterator')

    aws kinesis get-records --shard-iterator $SHARD_ITERATOR \
        | jq -r '.Records[] | .Data | @base64d' \
        | jq -r '.'
}
compdef "_arguments \
    '1:environment arg:(dev staging live)'" \
    rr-data-pump-lambda-submitted-manuscripts

function rr-recent-recommendations() {
    if [[ $# -ne 1 ]]; then
        echo "Usage: rr-recent-recommendations (dev|staging|live)"
        return 1
    fi

    local recsEnv="${1}"
    aws-recs-login "${recsEnv}" > /dev/null

    awslogs get --no-group --no-stream --timestamp "/aws/lambda/recs-reviewers-recommender-lambda-${recsEnv}" -f 'ManuscriptService' \
        | gsed -r 's/.* Manuscript id: (.+)$/\1/g' \
        | grep -e '^[^ ]\+$'
}
compdef "_arguments \
    '1:environment arg:(dev staging live)'" \
    rr-recent-recommendations

function rr-lambda-invocations() {
    if [[ $# -ne 1 ]]; then
        echo "Usage: rr-lambda-invocations (dev|staging|live)"
        return 1
    fi

    local recsEnv="${1}"
    aws-recs-login "${recsEnv}" > /dev/null

    lambdas=(
        recs-rev-reviewers-data-pump-lambda-${recsEnv}
        recs-rev-manuscripts-data-pump-lambda-${recsEnv}
        recs-reviewers-recommender-lambda-${recsEnv}
    )

    for lambda in "${lambdas[@]}"
    do
        echo "${lambda}"
        aws cloudwatch get-metric-statistics \
            --namespace 'AWS/Lambda' \
            --dimensions Name=FunctionName,Value="${lambda}" \
            --metric-name 'Invocations' \
            --start-time $(date --iso-8601=seconds --date='7 days ago') \
            --end-time   $(date --iso-8601=seconds) \
            --period $(calc '60 * 60 * 24') \
            --statistics Sum \
            | jq -r '["Time", "Total"], (.Datapoints | sort_by(.Timestamp) | .[] | [.Timestamp, .Sum]) | @csv' \
            | gsed 's/"//g' \
            | gsed "s/,,/,-,/g" \
            | column -t -s ','
        echo
    done
}
compdef "_arguments \
    '1:environment arg:(dev staging live)'" \
    rr-lambda-invocations

function rr-lambda-backlog() {
    if [[ $# -ne 1 ]]; then
        echo "Usage: rr-lambda-backlog (dev|staging|live)"
        return 1
    fi

    local recsEnv="${1}"
    aws-recs-login "${recsEnv}" > /dev/null

    local _lambda-invocations() {
        local lambda="${1}"

        aws cloudwatch get-metric-statistics \
            --namespace 'AWS/Lambda' \
            --dimensions Name=FunctionName,Value="${lambda}" \
            --metric-name 'Invocations' \
            --start-time $(date --iso-8601=seconds --date='7 days ago') \
            --end-time   $(date --iso-8601=seconds) \
            --period $(calc '60 * 60') \
            --statistics Sum \
            | jq -r '(.Datapoints | sort_by(.Timestamp) | .[] | [.Timestamp, .Sum]) | @csv' \
            | prepend "Time,Total"
    }

    local pumpDataFilename=.data-pump-invocations.csv
    local lambdaDataFilename=.lambda-invocations.csv
    local backlogDataFilename=backlog.txt
    local backlogImageFilename=backlog.png

    _lambda-invocations recs-rev-manuscripts-data-pump-lambda-${recsEnv} > ${pumpDataFilename}
    _lambda-invocations recs-reviewers-recommender-lambda-${recsEnv}     > ${lambdaDataFilename}

    local joinScript
    read-heredoc joinScript <<HEREDOC
        SELECT
            dp.Time AS time,
            dp.Total AS pump_count,
            l.Total AS lambda_count
        FROM '${pumpDataFilename}' dp INNER JOIN '${lambdaDataFilename}' l
        ON dp.Time = l.Time
HEREDOC

    local metricsScript
    read-heredoc metricsScript <<HEREDOC
        SELECT
            time,
            pump_count,
            sum(pump_count) over (ORDER BY time ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as pump_total,
            lambda_count,
            sum(lambda_count) over (ORDER BY time ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as lambda_total
        FROM read_csv('/dev/stdin')
HEREDOC

    duckdb -csv -c "${joinScript}" \
        | duckdb -csv -c "${metricsScript}" \
        | duckdb -csv -c "SELECT time, pump_total, lambda_total, pump_total - lambda_total as backlog_size FROM read_csv('/dev/stdin')" \
        | tabulate-by-comma \
        > backlog.txt

    local gnuplotScript
    read-heredoc gnuplotScript <<HEREDOC
        set style data line
        set xdata time
        set timefmt "%Y-%m-%d %H:%M:%S+00:00"
        set terminal png size 800,600 enhanced
        set output 'backlog.png'
        plot \
            "backlog.txt" using 1:2 title "Pump total"   linewidth 3, \
            "backlog.txt" using 1:3 title "Lambda total" linewidth 3, \
            "backlog.txt" using 1:4 title "Delta"        linewidth 3
HEREDOC

    echo ${gnuplotScript} | gnuplot
    imgcat backlog.png

    # Graph backlog separately so we can see where it's up to
    read-heredoc gnuplotScript <<HEREDOC
        set style data line
        set xdata time
        set timefmt "%Y-%m-%d %H:%M:%S+00:00"
        set terminal png size 800,600 enhanced
        set output 'backlog.png'
        plot \
            "backlog.txt" using 1:5 title "Delta" linewidth 3
HEREDOC

    echo ${gnuplotScript} | gnuplot
    imgcat backlog.png

    rm ${pumpDataFilename}
    rm ${lambdaDataFilename}
    rm ${backlogDataFilename}
    rm ${backlogImageFilename}
}
compdef "_arguments \
    '1:environment arg:(dev staging live)'" \
    rr-lambda-backlog

# Azure                                                                     {{{1
# ==============================================================================

function az-list-models-for-project() {
    if [[ $# -ne 1 ]]; then
        echo "Usage: az-list-models-for-project (innovation|sebright)"
        return 1
    fi

    local project=$1

    [ "${project}" = 'sebright' ] && SUBSCRIPTIONS=(
        az-ag-ScopusGPT-backup
        az-ag-ScopusGPT-nonprod
        az-ag-ScopusGPT-prod
    )

    [ "${project}" = 'innovation' ] && SUBSCRIPTIONS=(
        az-ag-Hackathon1-nonprod
        az-ag-Hackathon2-nonprod
    )

    for subscription in "${SUBSCRIPTIONS[@]}"; do
        echo "--"
        echo "-- ${subscription}"
        echo "--"
        echo
        az account set --subscription "${subscription}"

        while IFS=, read -rA azureInfo; do
            IFS=' ' read -rA azureFields <<< "${azureInfo}"
            resourceGroup="${azureFields[1]}"
            name="${azureFields[2]}"

            echo "${name}"
            az cognitiveservices account deployment list --resource-group "${resourceGroup}" --name "${name}" --output json \
                | jq -r '.[] | .properties | [ (.model | .name, .version), (.rateLimits[] | select(.key == "request").count,select(.key == "token").count  ) ] | @csv' \
                | gsed 's/"//g' \
                | sort \
                | prepend "model,version,request_limit,token_limit" \
                | tabulate-by-comma
            echo

        done < <(az cognitiveservices account list --output json | jq -r '.[] | [ .resourceGroup, .name ] | @csv' | sed 's/"//g' | sort)
    done
}
compdef "_arguments \
    '1:environment arg:(innovation sebright)'" \
    az-list-models-for-project

function az-list-model-limits() {
    az cognitiveservices model list --location eastus --output json \
        | jq -r '.[] | .model.name as $modelName | .model.skus[] | .name as $skuName | .capacity.maximum as $capMax | .rateLimits[] | [$modelName, $skuName, $capMax, .key, .count] | @csv' \
        | gsed 's/"//g' \
        | prepend 'model,sku,max,limit,value' \
        | tabulate-by-comma
}

function az-generate-model-stats() {
    regions=$(az account list-locations --output json | jq -r ".[].name")

    echo "Generating .az-model-stats.csv"
    echo '"region", "kind", "model", "version", "skus", "deprecation"' > .az-model-stats.csv
    while read -rA region; do
        az cognitiveservices model list --location ${region} --output json \
            | jq -r --arg region ${region} '.[] | .model.skus[].name as $skusName | [$region, .kind, .model.name, .model.version, $skusName, .model.deprecation.inference] | @csv'
    done <<< "${regions}" >> .az-model-stats.csv
}

function az-stats-model-regions() {
    local query
    read-heredoc query <<HEREDOC
    SELECT model, version, deprecation, count(region) AS total_regions, group_concat(region, '; ') AS regions 
    FROM .az-model-stats.csv
    WHERE model LIKE '%gpt%' 
        AND kind IS 'OpenAI'
        AND skus IS 'Standard'
    GROUP BY model,version,deprecation
HEREDOC

    echo 'Regions for OpenAI GPT models with standard SKUs'
    q "${query}" | tabulate-by-comma
}

# Terraform                                                                 {{{1
# ==============================================================================

# Apple silicon Terraform providers
TFENV_ARCH=amd64
